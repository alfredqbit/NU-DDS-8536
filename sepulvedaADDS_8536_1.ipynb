{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfredqbit/NU-DDS-8536/blob/main/sepulvedaADDS_8536_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF_fyhhLihOI"
      },
      "source": [
        "Copyright & Use Notice\n",
        "© 2026 Alfredo Sepulveda-Jimenez, QDR Labs. All rights reserved.\n",
        "\n",
        "Unless otherwise indicated, the original software source code and original explanatory text in this Jupyter Notebook (the “Work”) are owned by Alfred Sepulveda-Jimenez, QDR Labs and are protected by copyright and other applicable intellectual property laws.\n",
        "\n",
        "No license or other rights are granted under this notice. You may not use, copy, modify, merge, publish, distribute, sublicense, transmit, display, perform, or create derivative works of the Work, in whole or in part, except (i) as expressly permitted by applicable law, or (ii) with the prior written permission of Alfred Sepulveda-Jimenez, QDR Labs.\n",
        "\n",
        "Third‑party software, datasets, figures, and other materials referenced, embedded, or generated using external sources are the property of their respective owners and are subject to their own licenses and terms. Such materials are not licensed to you under this notice.\n",
        "\n",
        "To the maximum extent permitted by law, the Work is provided “AS IS”, without warranty of any kind, express or implied, including but not limited to warranties of merchantability, fitness for a particular purpose, and non‑infringement. To the maximum extent permitted by law, in no event shall Alfred Sepulveda-Jimenez, QDR Labs be liable for any claim, damages, or other liability, whether in an action of contract, tort, or otherwise, arising from, out of, or in connection with the Work or the use or other dealings in the Work.\n",
        "\n",
        "Any permitted copies must retain this notice.\n",
        "## DDS-8536 Assignment 1: Clustering Analysis: K-Means vs. DBSCAN, Quantum-inspired Alternatives on Wine Quality Dataset\n",
        "### Objectives\n",
        "1. Perform EDA on a univariate variable (alcohol) with >500 observations.\n",
        "2. Apply K-Means clustering (1D) and visualize results.\n",
        "3. Apply DBSCAN clustering (1D), tune parameters, and compare with K-Means.\n",
        "4. Apply both algorithms to a higher-dimensional dataset (11 physicochemical features).\n",
        "5. Save figures to a local `figures/` folder for inclusion in the LaTeX paper.\n",
        "\n",
        "---\n",
        "\n",
        "### Table of Contents\n",
        "1. Introduction and Dataset Selection\n",
        "2. Exploratory Data Analysis (Univariate)\n",
        "3. K-Means Clustering\n",
        "4. DBSCAN Clustering\n",
        "5. Comparative Analysis\n",
        "6. High-Dimensional Dataset Analysis\n",
        "7. Quantum-Inspired Clustering\n",
        "8. Proposed Extensions and Conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:15.471600Z",
          "iopub.status.busy": "2026-01-26T17:40:15.471345Z",
          "iopub.status.idle": "2026-01-26T17:40:18.707001Z",
          "shell.execute_reply": "2026-01-26T17:40:18.705511Z"
        },
        "id": "NwFAD3n_ihOL"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import cdist\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for visualizations\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('husl')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBsXF-5IihON"
      },
      "source": [
        "## 1. Dataset Selection and Loading\n",
        "\n",
        "### Primary Dataset: UCI Wine Quality Dataset\n",
        "\n",
        "**Source:** UCI Machine Learning Repository  \n",
        "**URL:** https://archive.ics.uci.edu/dataset/186/wine+quality  \n",
        "\n",
        "We use the **White Wine Quality** dataset containing 4,898 samples with 11 physicochemical properties. For univariate analysis, we focus on the **alcohol** content variable. For high-dimensional analysis, we use all 11 features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:18.754979Z",
          "iopub.status.busy": "2026-01-26T17:40:18.754500Z",
          "iopub.status.idle": "2026-01-26T17:40:18.876839Z",
          "shell.execute_reply": "2026-01-26T17:40:18.875395Z"
        },
        "id": "F82au4r-ihON"
      },
      "outputs": [],
      "source": [
        "# Load the Wine Quality dataset\n",
        "# Download URL: https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/\n",
        "\n",
        "url_white = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
        "\n",
        "try:\n",
        "    df_wine = pd.read_csv(url_white, sep=';')\n",
        "    print(f\"Dataset loaded successfully!\")\n",
        "    print(f\"Shape: {df_wine.shape}\")\n",
        "except:\n",
        "    # Fallback: Generate synthetic wine-like data\n",
        "    np.random.seed(42)\n",
        "    n_samples = 4898\n",
        "    df_wine = pd.DataFrame({\n",
        "        'fixed acidity': np.random.normal(6.85, 0.84, n_samples),\n",
        "        'volatile acidity': np.random.normal(0.28, 0.1, n_samples),\n",
        "        'citric acid': np.random.normal(0.33, 0.12, n_samples),\n",
        "        'residual sugar': np.random.exponential(5.0, n_samples) + 1,\n",
        "        'chlorides': np.random.normal(0.046, 0.02, n_samples),\n",
        "        'free sulfur dioxide': np.random.normal(35, 17, n_samples),\n",
        "        'total sulfur dioxide': np.random.normal(138, 42, n_samples),\n",
        "        'density': np.random.normal(0.994, 0.003, n_samples),\n",
        "        'pH': np.random.normal(3.19, 0.15, n_samples),\n",
        "        'sulphates': np.random.normal(0.49, 0.11, n_samples),\n",
        "        'alcohol': np.concatenate([\n",
        "            np.random.normal(9.5, 0.8, n_samples//3),\n",
        "            np.random.normal(11.0, 0.7, n_samples//3),\n",
        "            np.random.normal(12.5, 0.6, n_samples - 2*(n_samples//3))\n",
        "        ]),\n",
        "        'quality': np.random.choice([3,4,5,6,7,8,9], n_samples, p=[0.01,0.03,0.30,0.45,0.18,0.02,0.01])\n",
        "    })\n",
        "    print(\"Using synthetic wine-like data\")\n",
        "    print(f\"Shape: {df_wine.shape}\")\n",
        "\n",
        "# Display basic info\n",
        "print(\"\\nDataset Columns:\")\n",
        "print(df_wine.columns.tolist())\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "df_wine.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOqO5qPIihOO"
      },
      "source": [
        "---\n",
        "## 2. Exploratory Data Analysis (EDA) - Univariate\n",
        "\n",
        "For univariate clustering analysis, we select the **alcohol** variable, which shows interesting multimodal characteristics suitable for clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:18.880202Z",
          "iopub.status.busy": "2026-01-26T17:40:18.879906Z",
          "iopub.status.idle": "2026-01-26T17:40:18.890548Z",
          "shell.execute_reply": "2026-01-26T17:40:18.889315Z"
        },
        "id": "K0gyWkssihOO"
      },
      "outputs": [],
      "source": [
        "# Extract univariate data\n",
        "univariate_data = df_wine['alcohol'].values\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"UNIVARIATE EDA: Alcohol Content (% by volume)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Descriptive statistics\n",
        "stats_dict = {\n",
        "    'Count': len(univariate_data),\n",
        "    'Mean': np.mean(univariate_data),\n",
        "    'Median': np.median(univariate_data),\n",
        "    'Std Dev': np.std(univariate_data),\n",
        "    'Variance': np.var(univariate_data),\n",
        "    'Min': np.min(univariate_data),\n",
        "    'Max': np.max(univariate_data),\n",
        "    'Range': np.max(univariate_data) - np.min(univariate_data),\n",
        "    'IQR': np.percentile(univariate_data, 75) - np.percentile(univariate_data, 25),\n",
        "    'Skewness': stats.skew(univariate_data),\n",
        "    'Kurtosis': stats.kurtosis(univariate_data)\n",
        "}\n",
        "\n",
        "for key, value in stats_dict.items():\n",
        "    print(f\"{key:15}: {value:.4f}\")\n",
        "\n",
        "# Normality tests\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"NORMALITY TESTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Shapiro-Wilk test (sample of 5000)\n",
        "sample_size = min(5000, len(univariate_data))\n",
        "_, shapiro_p = stats.shapiro(np.random.choice(univariate_data, sample_size, replace=False))\n",
        "print(f\"Shapiro-Wilk p-value: {shapiro_p:.6f}\")\n",
        "print(f\"Interpretation: {'Normal' if shapiro_p > 0.05 else 'Non-normal'} distribution\")\n",
        "\n",
        "# D'Agostino-Pearson test\n",
        "_, dagostino_p = stats.normaltest(univariate_data)\n",
        "print(f\"D'Agostino-Pearson p-value: {dagostino_p:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:18.893334Z",
          "iopub.status.busy": "2026-01-26T17:40:18.893086Z",
          "iopub.status.idle": "2026-01-26T17:40:20.809272Z",
          "shell.execute_reply": "2026-01-26T17:40:20.807971Z"
        },
        "id": "HWipfCanihOO"
      },
      "outputs": [],
      "source": [
        "# Comprehensive visualization of univariate data\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "\n",
        "# 1. Histogram with KDE\n",
        "ax1 = axes[0, 0]\n",
        "ax1.hist(univariate_data, bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "kde_x = np.linspace(univariate_data.min(), univariate_data.max(), 200)\n",
        "kde = stats.gaussian_kde(univariate_data)\n",
        "ax1.plot(kde_x, kde(kde_x), 'r-', lw=2, label='KDE')\n",
        "ax1.axvline(np.mean(univariate_data), color='green', linestyle='--', lw=2, label=f'Mean: {np.mean(univariate_data):.2f}')\n",
        "ax1.axvline(np.median(univariate_data), color='orange', linestyle='-.', lw=2, label=f'Median: {np.median(univariate_data):.2f}')\n",
        "ax1.set_xlabel('Alcohol Content (%)', fontsize=12)\n",
        "ax1.set_ylabel('Density', fontsize=12)\n",
        "ax1.set_title('Histogram with KDE', fontsize=14, fontweight='bold')\n",
        "ax1.legend(fontsize=10)\n",
        "\n",
        "# 2. Box Plot\n",
        "ax2 = axes[0, 1]\n",
        "bp = ax2.boxplot(univariate_data, vert=True, patch_artist=True)\n",
        "bp['boxes'][0].set_facecolor('lightblue')\n",
        "ax2.set_ylabel('Alcohol Content (%)', fontsize=12)\n",
        "ax2.set_title('Box Plot', fontsize=14, fontweight='bold')\n",
        "# Add quartile annotations\n",
        "q1, q2, q3 = np.percentile(univariate_data, [25, 50, 75])\n",
        "ax2.annotate(f'Q1: {q1:.2f}', xy=(1.1, q1), fontsize=10)\n",
        "ax2.annotate(f'Q2: {q2:.2f}', xy=(1.1, q2), fontsize=10)\n",
        "ax2.annotate(f'Q3: {q3:.2f}', xy=(1.1, q3), fontsize=10)\n",
        "\n",
        "# 3. Violin Plot\n",
        "ax3 = axes[0, 2]\n",
        "parts = ax3.violinplot(univariate_data, vert=True, showmeans=True, showmedians=True)\n",
        "for pc in parts['bodies']:\n",
        "    pc.set_facecolor('lightcoral')\n",
        "    pc.set_alpha(0.7)\n",
        "ax3.set_ylabel('Alcohol Content (%)', fontsize=12)\n",
        "ax3.set_title('Violin Plot', fontsize=14, fontweight='bold')\n",
        "\n",
        "# 4. Q-Q Plot\n",
        "ax4 = axes[1, 0]\n",
        "stats.probplot(univariate_data, dist=\"norm\", plot=ax4)\n",
        "ax4.set_title('Q-Q Plot (Normality Check)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# 5. ECDF\n",
        "ax5 = axes[1, 1]\n",
        "sorted_data = np.sort(univariate_data)\n",
        "ecdf = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
        "ax5.step(sorted_data, ecdf, where='post', color='navy', lw=2)\n",
        "ax5.set_xlabel('Alcohol Content (%)', fontsize=12)\n",
        "ax5.set_ylabel('Cumulative Probability', fontsize=12)\n",
        "ax5.set_title('Empirical CDF', fontsize=14, fontweight='bold')\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Strip/Swarm plot (sample for visibility)\n",
        "ax6 = axes[1, 2]\n",
        "sample_idx = np.random.choice(len(univariate_data), min(500, len(univariate_data)), replace=False)\n",
        "ax6.scatter(np.ones(len(sample_idx)) + np.random.normal(0, 0.05, len(sample_idx)),\n",
        "            univariate_data[sample_idx], alpha=0.5, s=10, color='purple')\n",
        "ax6.set_xlim(0.5, 1.5)\n",
        "ax6.set_ylabel('Alcohol Content (%)', fontsize=12)\n",
        "ax6.set_title('Strip Plot (n=500 sample)', fontsize=14, fontweight='bold')\n",
        "ax6.set_xticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('eda_univariate.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nEDA visualization saved as 'eda_univariate.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdStBLBnihOP"
      },
      "source": [
        "### EDA Insights\n",
        "\n",
        "The alcohol content distribution shows:\n",
        "1. **Mild right skewness** suggesting presence of subgroups\n",
        "2. **Multiple potential modes** visible in the KDE\n",
        "3. **Some outliers** in the high-alcohol range\n",
        "4. **Deviation from normality** as evidenced by Q-Q plot curvature\n",
        "\n",
        "These characteristics make this variable suitable for clustering analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEROxN7IihOP"
      },
      "source": [
        "---\n",
        "## 3. K-Means Clustering (Univariate)\n",
        "\n",
        "### 3.1 Optimal K Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:20.813040Z",
          "iopub.status.busy": "2026-01-26T17:40:20.812748Z",
          "iopub.status.idle": "2026-01-26T17:40:24.565881Z",
          "shell.execute_reply": "2026-01-26T17:40:24.564027Z"
        },
        "id": "na_BUsPZihOP"
      },
      "outputs": [],
      "source": [
        "# Prepare data for K-Means\n",
        "X_univariate = univariate_data.reshape(-1, 1)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_univariate)\n",
        "\n",
        "# Determine optimal K using multiple methods\n",
        "K_range = range(2, 11)\n",
        "inertias = []\n",
        "silhouettes = []\n",
        "davies_bouldin = []\n",
        "calinski_harabasz = []\n",
        "\n",
        "print(\"Evaluating K-Means for K = 2 to 10...\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'K':>3} | {'Inertia':>12} | {'Silhouette':>10} | {'Davies-Bouldin':>14} | {'Calinski-Harabasz':>17}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
        "    labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    sil = silhouette_score(X_scaled, labels)\n",
        "    silhouettes.append(sil)\n",
        "    db = davies_bouldin_score(X_scaled, labels)\n",
        "    davies_bouldin.append(db)\n",
        "    ch = calinski_harabasz_score(X_scaled, labels)\n",
        "    calinski_harabasz.append(ch)\n",
        "\n",
        "    print(f\"{k:>3} | {kmeans.inertia_:>12.2f} | {sil:>10.4f} | {db:>14.4f} | {ch:>17.2f}\")\n",
        "\n",
        "# Optimal K analysis\n",
        "optimal_k_silhouette = K_range[np.argmax(silhouettes)]\n",
        "optimal_k_db = K_range[np.argmin(davies_bouldin)]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"Optimal K (Silhouette): {optimal_k_silhouette}\")\n",
        "print(f\"Optimal K (Davies-Bouldin): {optimal_k_db}\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:24.569454Z",
          "iopub.status.busy": "2026-01-26T17:40:24.569098Z",
          "iopub.status.idle": "2026-01-26T17:40:26.172266Z",
          "shell.execute_reply": "2026-01-26T17:40:26.170766Z"
        },
        "id": "nm1pc08hihOP"
      },
      "outputs": [],
      "source": [
        "# Visualization of K selection criteria\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Elbow Method\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(list(K_range), inertias, 'bo-', markersize=10, linewidth=2)\n",
        "ax1.set_xlabel('Number of Clusters (K)', fontsize=12)\n",
        "ax1.set_ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12)\n",
        "ax1.set_title('Elbow Method', fontsize=14, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Silhouette Score\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(list(K_range), silhouettes, 'go-', markersize=10, linewidth=2)\n",
        "ax2.axvline(x=optimal_k_silhouette, color='red', linestyle='--', label=f'Optimal K={optimal_k_silhouette}')\n",
        "ax2.set_xlabel('Number of Clusters (K)', fontsize=12)\n",
        "ax2.set_ylabel('Silhouette Score', fontsize=12)\n",
        "ax2.set_title('Silhouette Analysis (Higher is Better)', fontsize=14, fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Davies-Bouldin Index\n",
        "ax3 = axes[1, 0]\n",
        "ax3.plot(list(K_range), davies_bouldin, 'ro-', markersize=10, linewidth=2)\n",
        "ax3.axvline(x=optimal_k_db, color='blue', linestyle='--', label=f'Optimal K={optimal_k_db}')\n",
        "ax3.set_xlabel('Number of Clusters (K)', fontsize=12)\n",
        "ax3.set_ylabel('Davies-Bouldin Index', fontsize=12)\n",
        "ax3.set_title('Davies-Bouldin Index (Lower is Better)', fontsize=14, fontweight='bold')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Calinski-Harabasz Index\n",
        "ax4 = axes[1, 1]\n",
        "ax4.plot(list(K_range), calinski_harabasz, 'mo-', markersize=10, linewidth=2)\n",
        "ax4.set_xlabel('Number of Clusters (K)', fontsize=12)\n",
        "ax4.set_ylabel('Calinski-Harabasz Index', fontsize=12)\n",
        "ax4.set_title('Calinski-Harabasz Index (Higher is Better)', fontsize=14, fontweight='bold')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('kmeans_k_selection.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:26.175334Z",
          "iopub.status.busy": "2026-01-26T17:40:26.175047Z",
          "iopub.status.idle": "2026-01-26T17:40:26.616678Z",
          "shell.execute_reply": "2026-01-26T17:40:26.614961Z"
        },
        "id": "c3nXByEnihOQ"
      },
      "outputs": [],
      "source": [
        "# Apply K-Means with optimal K\n",
        "OPTIMAL_K = 3  # Based on combined analysis\n",
        "\n",
        "kmeans_final = KMeans(n_clusters=OPTIMAL_K, random_state=42, n_init=10)\n",
        "kmeans_labels = kmeans_final.fit_predict(X_scaled)\n",
        "\n",
        "# Get centroids (inverse transform to original scale)\n",
        "centroids_scaled = kmeans_final.cluster_centers_\n",
        "centroids_original = scaler.inverse_transform(centroids_scaled)\n",
        "\n",
        "print(f\"\\nK-Means Clustering Results (K={OPTIMAL_K})\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\nCluster Centroids (Alcohol %):\\n\")\n",
        "for i, centroid in enumerate(centroids_original):\n",
        "    count = np.sum(kmeans_labels == i)\n",
        "    print(f\"  Cluster {i}: Centroid = {centroid[0]:.2f}%, n = {count} ({100*count/len(kmeans_labels):.1f}%)\")\n",
        "\n",
        "print(f\"\\nOverall Silhouette Score: {silhouette_score(X_scaled, kmeans_labels):.4f}\")\n",
        "print(f\"Davies-Bouldin Index: {davies_bouldin_score(X_scaled, kmeans_labels):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:26.619720Z",
          "iopub.status.busy": "2026-01-26T17:40:26.619302Z",
          "iopub.status.idle": "2026-01-26T17:40:28.209019Z",
          "shell.execute_reply": "2026-01-26T17:40:28.207369Z"
        },
        "id": "TcWHnmjgihOQ"
      },
      "outputs": [],
      "source": [
        "# Visualization of K-Means results\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, OPTIMAL_K))\n",
        "\n",
        "# 1. Histogram by cluster\n",
        "ax1 = axes[0]\n",
        "for i in range(OPTIMAL_K):\n",
        "    cluster_data = univariate_data[kmeans_labels == i]\n",
        "    ax1.hist(cluster_data, bins=30, alpha=0.6, color=colors[i],\n",
        "             label=f'Cluster {i} (n={len(cluster_data)})', edgecolor='black')\n",
        "    ax1.axvline(centroids_original[i], color=colors[i], linestyle='--', linewidth=2)\n",
        "ax1.set_xlabel('Alcohol Content (%)', fontsize=12)\n",
        "ax1.set_ylabel('Frequency', fontsize=12)\n",
        "ax1.set_title('K-Means Cluster Distribution', fontsize=14, fontweight='bold')\n",
        "ax1.legend()\n",
        "\n",
        "# 2. Scatter plot (1D projected to 2D for visualization)\n",
        "ax2 = axes[1]\n",
        "y_jitter = np.random.normal(0, 0.1, len(univariate_data))\n",
        "scatter = ax2.scatter(univariate_data, y_jitter, c=kmeans_labels, cmap='viridis',\n",
        "                      alpha=0.6, s=20)\n",
        "ax2.scatter(centroids_original, [0]*OPTIMAL_K, c='red', s=200, marker='X',\n",
        "            edgecolors='black', linewidths=2, label='Centroids', zorder=5)\n",
        "ax2.set_xlabel('Alcohol Content (%)', fontsize=12)\n",
        "ax2.set_ylabel('Jittered (for visualization)', fontsize=12)\n",
        "ax2.set_title('K-Means Cluster Assignment', fontsize=14, fontweight='bold')\n",
        "ax2.legend()\n",
        "plt.colorbar(scatter, ax=ax2, label='Cluster')\n",
        "\n",
        "# 3. Box plot by cluster\n",
        "ax3 = axes[2]\n",
        "cluster_data_list = [univariate_data[kmeans_labels == i] for i in range(OPTIMAL_K)]\n",
        "bp = ax3.boxplot(cluster_data_list, patch_artist=True)\n",
        "for patch, color in zip(bp['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "ax3.set_xlabel('Cluster', fontsize=12)\n",
        "ax3.set_ylabel('Alcohol Content (%)', fontsize=12)\n",
        "ax3.set_title('Cluster Statistics', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('kmeans_results_univariate.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXuIJhI7ihOQ"
      },
      "source": [
        "---\n",
        "## 4. DBSCAN Clustering (Univariate)\n",
        "\n",
        "### 4.1 Parameter Selection using k-distance Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:28.212724Z",
          "iopub.status.busy": "2026-01-26T17:40:28.212420Z",
          "iopub.status.idle": "2026-01-26T17:40:28.737585Z",
          "shell.execute_reply": "2026-01-26T17:40:28.736209Z"
        },
        "id": "efmIrjkeihOQ"
      },
      "outputs": [],
      "source": [
        "# Determine optimal epsilon using k-distance graph\n",
        "# For univariate data, min_samples typically 2-4\n",
        "\n",
        "k = 4  # min_samples candidate\n",
        "neighbors = NearestNeighbors(n_neighbors=k)\n",
        "neighbors.fit(X_scaled)\n",
        "distances, _ = neighbors.kneighbors(X_scaled)\n",
        "distances = np.sort(distances[:, k-1])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(range(len(distances)), distances, 'b-', linewidth=1)\n",
        "ax.set_xlabel('Points (sorted by distance)', fontsize=12)\n",
        "ax.set_ylabel(f'{k}-th Nearest Neighbor Distance', fontsize=12)\n",
        "ax.set_title('k-Distance Graph for Epsilon Selection', fontsize=14, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Find elbow point (simple gradient method)\n",
        "gradient = np.gradient(distances)\n",
        "elbow_idx = np.argmax(gradient > np.mean(gradient) + 2*np.std(gradient))\n",
        "if elbow_idx == 0:\n",
        "    elbow_idx = int(0.95 * len(distances))  # fallback\n",
        "suggested_eps = distances[elbow_idx]\n",
        "\n",
        "ax.axhline(y=suggested_eps, color='red', linestyle='--',\n",
        "           label=f'Suggested ε ≈ {suggested_eps:.3f}')\n",
        "ax.legend(fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('dbscan_epsilon_selection.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Suggested epsilon: {suggested_eps:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:28.741809Z",
          "iopub.status.busy": "2026-01-26T17:40:28.741488Z",
          "iopub.status.idle": "2026-01-26T17:40:30.889835Z",
          "shell.execute_reply": "2026-01-26T17:40:30.888038Z"
        },
        "id": "dV3iXTc_ihOR"
      },
      "outputs": [],
      "source": [
        "# Parameter grid search for DBSCAN\n",
        "eps_range = np.arange(0.1, 0.8, 0.1)\n",
        "min_samples_range = [3, 4, 5, 10, 15, 20]\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"DBSCAN Parameter Search\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'eps':>6} | {'min_samples':>11} | {'n_clusters':>10} | {'noise_pts':>10} | {'silhouette':>10}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for eps in eps_range:\n",
        "    for min_samples in min_samples_range:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "        n_noise = np.sum(labels == -1)\n",
        "\n",
        "        if n_clusters >= 2:\n",
        "            # Calculate silhouette only for non-noise points\n",
        "            mask = labels != -1\n",
        "            if np.sum(mask) > 1:\n",
        "                sil = silhouette_score(X_scaled[mask], labels[mask])\n",
        "            else:\n",
        "                sil = -1\n",
        "        else:\n",
        "            sil = -1\n",
        "\n",
        "        results.append({\n",
        "            'eps': eps, 'min_samples': min_samples,\n",
        "            'n_clusters': n_clusters, 'n_noise': n_noise, 'silhouette': sil\n",
        "        })\n",
        "\n",
        "        if n_clusters >= 2 and sil > 0:\n",
        "            print(f\"{eps:>6.2f} | {min_samples:>11} | {n_clusters:>10} | {n_noise:>10} | {sil:>10.4f}\")\n",
        "\n",
        "# Find best parameters\n",
        "valid_results = [r for r in results if r['n_clusters'] >= 2 and r['silhouette'] > 0]\n",
        "if valid_results:\n",
        "    best = max(valid_results, key=lambda x: x['silhouette'])\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Best Parameters: eps={best['eps']:.2f}, min_samples={best['min_samples']}\")\n",
        "    print(f\"Clusters: {best['n_clusters']}, Noise points: {best['n_noise']}, Silhouette: {best['silhouette']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:30.892975Z",
          "iopub.status.busy": "2026-01-26T17:40:30.892684Z",
          "iopub.status.idle": "2026-01-26T17:40:30.931802Z",
          "shell.execute_reply": "2026-01-26T17:40:30.930218Z"
        },
        "id": "dc7RTovrihOR"
      },
      "outputs": [],
      "source": [
        "# Apply DBSCAN with selected parameters\n",
        "EPS_OPTIMAL = 0.3\n",
        "MIN_SAMPLES_OPTIMAL = 10\n",
        "\n",
        "dbscan_final = DBSCAN(eps=EPS_OPTIMAL, min_samples=MIN_SAMPLES_OPTIMAL)\n",
        "dbscan_labels = dbscan_final.fit_predict(X_scaled)\n",
        "\n",
        "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
        "n_noise = np.sum(dbscan_labels == -1)\n",
        "\n",
        "print(f\"\\nDBSCAN Clustering Results\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Parameters: eps={EPS_OPTIMAL}, min_samples={MIN_SAMPLES_OPTIMAL}\")\n",
        "print(f\"Number of clusters: {n_clusters_dbscan}\")\n",
        "print(f\"Noise points: {n_noise} ({100*n_noise/len(dbscan_labels):.1f}%)\")\n",
        "\n",
        "print(\"\\nCluster Statistics:\")\n",
        "for label in sorted(set(dbscan_labels)):\n",
        "    cluster_data = univariate_data[dbscan_labels == label]\n",
        "    label_name = \"Noise\" if label == -1 else f\"Cluster {label}\"\n",
        "    print(f\"  {label_name}: n={len(cluster_data)}, mean={np.mean(cluster_data):.2f}%, std={np.std(cluster_data):.2f}\")\n",
        "\n",
        "# Calculate metrics for non-noise points\n",
        "mask = dbscan_labels != -1\n",
        "if np.sum(mask) > 1 and n_clusters_dbscan >= 2:\n",
        "    sil_dbscan = silhouette_score(X_scaled[mask], dbscan_labels[mask])\n",
        "    db_dbscan = davies_bouldin_score(X_scaled[mask], dbscan_labels[mask])\n",
        "    print(f\"\\nSilhouette Score (excl. noise): {sil_dbscan:.4f}\")\n",
        "    print(f\"Davies-Bouldin Index (excl. noise): {db_dbscan:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:30.935492Z",
          "iopub.status.busy": "2026-01-26T17:40:30.935091Z",
          "iopub.status.idle": "2026-01-26T17:40:32.385877Z",
          "shell.execute_reply": "2026-01-26T17:40:32.383989Z"
        },
        "id": "CiGovU2AihOR"
      },
      "outputs": [],
      "source": [
        "# Visualization of DBSCAN results\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "unique_labels = sorted(set(dbscan_labels))\n",
        "colors_db = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))\n",
        "\n",
        "# 1. Histogram by cluster\n",
        "ax1 = axes[0]\n",
        "for i, label in enumerate(unique_labels):\n",
        "    cluster_data = univariate_data[dbscan_labels == label]\n",
        "    label_name = \"Noise\" if label == -1 else f\"Cluster {label}\"\n",
        "    ax1.hist(cluster_data, bins=30, alpha=0.6, color=colors_db[i],\n",
        "             label=f'{label_name} (n={len(cluster_data)})', edgecolor='black')\n",
        "ax1.set_xlabel('Alcohol Content (%)', fontsize=12)\n",
        "ax1.set_ylabel('Frequency', fontsize=12)\n",
        "ax1.set_title('DBSCAN Cluster Distribution', fontsize=14, fontweight='bold')\n",
        "ax1.legend()\n",
        "\n",
        "# 2. Scatter plot\n",
        "ax2 = axes[1]\n",
        "y_jitter = np.random.normal(0, 0.1, len(univariate_data))\n",
        "for i, label in enumerate(unique_labels):\n",
        "    mask = dbscan_labels == label\n",
        "    marker = 'x' if label == -1 else 'o'\n",
        "    alpha = 0.3 if label == -1 else 0.7\n",
        "    label_name = \"Noise\" if label == -1 else f\"Cluster {label}\"\n",
        "    ax2.scatter(univariate_data[mask], y_jitter[mask], c=[colors_db[i]],\n",
        "                alpha=alpha, s=20, marker=marker, label=label_name)\n",
        "ax2.set_xlabel('Alcohol Content (%)', fontsize=12)\n",
        "ax2.set_ylabel('Jittered (for visualization)', fontsize=12)\n",
        "ax2.set_title('DBSCAN Cluster Assignment', fontsize=14, fontweight='bold')\n",
        "ax2.legend()\n",
        "\n",
        "# 3. Comparison with K-Means\n",
        "ax3 = axes[2]\n",
        "ax3.scatter(univariate_data, kmeans_labels + np.random.normal(0, 0.05, len(kmeans_labels)),\n",
        "            alpha=0.5, s=10, label='K-Means', c='blue')\n",
        "ax3.scatter(univariate_data, dbscan_labels + 0.5 + np.random.normal(0, 0.05, len(dbscan_labels)),\n",
        "            alpha=0.5, s=10, label='DBSCAN', c='red')\n",
        "ax3.set_xlabel('Alcohol Content (%)', fontsize=12)\n",
        "ax3.set_ylabel('Cluster Label', fontsize=12)\n",
        "ax3.set_title('K-Means vs DBSCAN', fontsize=14, fontweight='bold')\n",
        "ax3.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('dbscan_results_univariate.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKfo1CzHihOR"
      },
      "source": [
        "---\n",
        "## 5. Comparative Analysis: K-Means vs DBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:32.389735Z",
          "iopub.status.busy": "2026-01-26T17:40:32.389355Z",
          "iopub.status.idle": "2026-01-26T17:40:32.814302Z",
          "shell.execute_reply": "2026-01-26T17:40:32.812816Z"
        },
        "id": "5gHtxIbtihOR"
      },
      "outputs": [],
      "source": [
        "# Comprehensive comparison table\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"COMPARATIVE ANALYSIS: K-Means vs DBSCAN (Univariate Data)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "comparison_data = {\n",
        "    'Metric': ['Number of Clusters', 'Noise Points', 'Silhouette Score',\n",
        "               'Davies-Bouldin Index', 'Parameters Required', 'Shape Assumption'],\n",
        "    'K-Means': [OPTIMAL_K, 0, f\"{silhouette_score(X_scaled, kmeans_labels):.4f}\",\n",
        "                f\"{davies_bouldin_score(X_scaled, kmeans_labels):.4f}\", 'K only', 'Spherical'],\n",
        "    'DBSCAN': [n_clusters_dbscan, n_noise,\n",
        "               f\"{silhouette_score(X_scaled[mask], dbscan_labels[mask]):.4f}\" if n_clusters_dbscan >= 2 else 'N/A',\n",
        "               f\"{davies_bouldin_score(X_scaled[mask], dbscan_labels[mask]):.4f}\" if n_clusters_dbscan >= 2 else 'N/A',\n",
        "               'eps, min_samples', 'Arbitrary']\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"KEY OBSERVATIONS:\")\n",
        "print(\"-\" * 70)\n",
        "observations = [\n",
        "    \"1. K-Means forces all points into clusters; DBSCAN identifies noise.\",\n",
        "    \"2. K-Means requires specifying K; DBSCAN determines clusters automatically.\",\n",
        "    \"3. For this univariate data, K-Means produces more distinct separation.\",\n",
        "    \"4. DBSCAN's noise detection helps identify outliers in alcohol content.\",\n",
        "    f\"5. DBSCAN classified {100*n_noise/len(dbscan_labels):.1f}% of data as noise/outliers.\"\n",
        "]\n",
        "for obs in observations:\n",
        "    print(obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJz4mwatihOS"
      },
      "source": [
        "---\n",
        "## 6. High-Dimensional Dataset Analysis\n",
        "\n",
        "Now we apply both algorithms to the full 11-dimensional wine quality dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:32.817813Z",
          "iopub.status.busy": "2026-01-26T17:40:32.817421Z",
          "iopub.status.idle": "2026-01-26T17:40:32.826139Z",
          "shell.execute_reply": "2026-01-26T17:40:32.824711Z"
        },
        "id": "4gOYJCulihOS"
      },
      "outputs": [],
      "source": [
        "# Prepare high-dimensional data\n",
        "feature_cols = [col for col in df_wine.columns if col != 'quality']\n",
        "X_high = df_wine[feature_cols].values\n",
        "\n",
        "# Scale the data\n",
        "scaler_high = StandardScaler()\n",
        "X_high_scaled = scaler_high.fit_transform(X_high)\n",
        "\n",
        "print(f\"High-dimensional dataset shape: {X_high_scaled.shape}\")\n",
        "print(f\"Features: {feature_cols}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:32.829034Z",
          "iopub.status.busy": "2026-01-26T17:40:32.828713Z",
          "iopub.status.idle": "2026-01-26T17:40:37.951992Z",
          "shell.execute_reply": "2026-01-26T17:40:37.950372Z"
        },
        "id": "TqRCFKyHihOS"
      },
      "outputs": [],
      "source": [
        "# K-Means on high-dimensional data\n",
        "K_range_high = range(2, 11)\n",
        "inertias_high = []\n",
        "silhouettes_high = []\n",
        "\n",
        "print(\"K-Means on High-Dimensional Data\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for k in K_range_high:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(X_high_scaled)\n",
        "    inertias_high.append(kmeans.inertia_)\n",
        "    sil = silhouette_score(X_high_scaled, labels)\n",
        "    silhouettes_high.append(sil)\n",
        "    print(f\"K={k}: Silhouette={sil:.4f}\")\n",
        "\n",
        "optimal_k_high = K_range_high[np.argmax(silhouettes_high)]\n",
        "print(f\"\\nOptimal K: {optimal_k_high}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:37.955396Z",
          "iopub.status.busy": "2026-01-26T17:40:37.955100Z",
          "iopub.status.idle": "2026-01-26T17:40:39.672431Z",
          "shell.execute_reply": "2026-01-26T17:40:39.671188Z"
        },
        "id": "1hGVdBlOihOS"
      },
      "outputs": [],
      "source": [
        "# Apply K-Means with optimal K\n",
        "OPTIMAL_K_HIGH = 3\n",
        "kmeans_high = KMeans(n_clusters=OPTIMAL_K_HIGH, random_state=42, n_init=10)\n",
        "kmeans_labels_high = kmeans_high.fit_predict(X_high_scaled)\n",
        "\n",
        "# PCA for visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_high_scaled)\n",
        "\n",
        "print(f\"PCA Explained Variance: {pca.explained_variance_ratio_.sum()*100:.1f}%\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# K-Means visualization\n",
        "ax1 = axes[0]\n",
        "scatter1 = ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels_high,\n",
        "                       cmap='viridis', alpha=0.6, s=20)\n",
        "centroids_pca = pca.transform(kmeans_high.cluster_centers_)\n",
        "ax1.scatter(centroids_pca[:, 0], centroids_pca[:, 1], c='red', s=200,\n",
        "            marker='X', edgecolors='black', linewidths=2, label='Centroids')\n",
        "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=12)\n",
        "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=12)\n",
        "ax1.set_title(f'K-Means (K={OPTIMAL_K_HIGH}) on High-D Data', fontsize=14, fontweight='bold')\n",
        "ax1.legend()\n",
        "plt.colorbar(scatter1, ax=ax1, label='Cluster')\n",
        "\n",
        "# Actual quality labels for comparison\n",
        "ax2 = axes[1]\n",
        "scatter2 = ax2.scatter(X_pca[:, 0], X_pca[:, 1], c=df_wine['quality'],\n",
        "                       cmap='plasma', alpha=0.6, s=20)\n",
        "ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=12)\n",
        "ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=12)\n",
        "ax2.set_title('Actual Wine Quality Ratings', fontsize=14, fontweight='bold')\n",
        "plt.colorbar(scatter2, ax=ax2, label='Quality')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('kmeans_high_dimensional.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:39.676431Z",
          "iopub.status.busy": "2026-01-26T17:40:39.676128Z",
          "iopub.status.idle": "2026-01-26T17:40:50.262008Z",
          "shell.execute_reply": "2026-01-26T17:40:50.260905Z"
        },
        "id": "gEWNNpzuihOS"
      },
      "outputs": [],
      "source": [
        "# DBSCAN on high-dimensional data\n",
        "# Need to adjust epsilon for higher dimensions\n",
        "\n",
        "k_high = 10  # Increase k for high-dimensional data\n",
        "neighbors_high = NearestNeighbors(n_neighbors=k_high)\n",
        "neighbors_high.fit(X_high_scaled)\n",
        "distances_high, _ = neighbors_high.kneighbors(X_high_scaled)\n",
        "distances_high = np.sort(distances_high[:, k_high-1])\n",
        "\n",
        "# Grid search for DBSCAN in high-D\n",
        "eps_range_high = np.arange(1.5, 4.0, 0.5)\n",
        "min_samples_high = [10, 15, 20, 30]\n",
        "\n",
        "print(\"DBSCAN Parameter Search (High-Dimensional)\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "best_sil_high = -1\n",
        "best_params_high = None\n",
        "\n",
        "for eps in eps_range_high:\n",
        "    for ms in min_samples_high:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=ms)\n",
        "        labels = dbscan.fit_predict(X_high_scaled)\n",
        "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "        n_noise = np.sum(labels == -1)\n",
        "\n",
        "        if n_clusters >= 2:\n",
        "            mask = labels != -1\n",
        "            if np.sum(mask) > 10:\n",
        "                sil = silhouette_score(X_high_scaled[mask], labels[mask])\n",
        "                if sil > best_sil_high:\n",
        "                    best_sil_high = sil\n",
        "                    best_params_high = {'eps': eps, 'min_samples': ms,\n",
        "                                        'n_clusters': n_clusters, 'n_noise': n_noise}\n",
        "                print(f\"eps={eps:.1f}, min_samples={ms}: clusters={n_clusters}, noise={n_noise}, sil={sil:.4f}\")\n",
        "\n",
        "print(f\"\\nBest: eps={best_params_high['eps']}, min_samples={best_params_high['min_samples']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:50.265829Z",
          "iopub.status.busy": "2026-01-26T17:40:50.265518Z",
          "iopub.status.idle": "2026-01-26T17:40:52.192537Z",
          "shell.execute_reply": "2026-01-26T17:40:52.190913Z"
        },
        "id": "zMV3tmcBihOS"
      },
      "outputs": [],
      "source": [
        "# Apply best DBSCAN parameters\n",
        "if best_params_high:\n",
        "    dbscan_high = DBSCAN(eps=best_params_high['eps'], min_samples=best_params_high['min_samples'])\n",
        "else:\n",
        "    dbscan_high = DBSCAN(eps=2.5, min_samples=15)\n",
        "\n",
        "dbscan_labels_high = dbscan_high.fit_predict(X_high_scaled)\n",
        "\n",
        "n_clusters_high = len(set(dbscan_labels_high)) - (1 if -1 in dbscan_labels_high else 0)\n",
        "n_noise_high = np.sum(dbscan_labels_high == -1)\n",
        "\n",
        "print(f\"DBSCAN Results (High-D): {n_clusters_high} clusters, {n_noise_high} noise points\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# DBSCAN visualization\n",
        "ax1 = axes[0]\n",
        "unique_labels_high = set(dbscan_labels_high)\n",
        "colors_high = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels_high)))\n",
        "\n",
        "for k, col in zip(unique_labels_high, colors_high):\n",
        "    if k == -1:\n",
        "        col = 'gray'\n",
        "        alpha = 0.2\n",
        "    else:\n",
        "        alpha = 0.7\n",
        "    class_member_mask = (dbscan_labels_high == k)\n",
        "    xy = X_pca[class_member_mask]\n",
        "    ax1.scatter(xy[:, 0], xy[:, 1], c=[col], alpha=alpha, s=20,\n",
        "                label=f'Cluster {k}' if k != -1 else 'Noise')\n",
        "\n",
        "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=12)\n",
        "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=12)\n",
        "ax1.set_title(f'DBSCAN on High-D Data', fontsize=14, fontweight='bold')\n",
        "ax1.legend()\n",
        "\n",
        "# Side-by-side comparison\n",
        "ax2 = axes[1]\n",
        "ax2.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels_high, cmap='viridis',\n",
        "            alpha=0.3, s=10, marker='o', label='K-Means')\n",
        "noise_mask = dbscan_labels_high == -1\n",
        "ax2.scatter(X_pca[noise_mask, 0], X_pca[noise_mask, 1], c='red',\n",
        "            alpha=0.5, s=30, marker='x', label=f'DBSCAN Noise (n={n_noise_high})')\n",
        "ax2.set_xlabel(f'PC1', fontsize=12)\n",
        "ax2.set_ylabel(f'PC2', fontsize=12)\n",
        "ax2.set_title('K-Means Clusters with DBSCAN Noise Points', fontsize=14, fontweight='bold')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('dbscan_high_dimensional.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:52.196824Z",
          "iopub.status.busy": "2026-01-26T17:40:52.196453Z",
          "iopub.status.idle": "2026-01-26T17:40:52.581696Z",
          "shell.execute_reply": "2026-01-26T17:40:52.580031Z"
        },
        "id": "kimtkB_PihOT"
      },
      "outputs": [],
      "source": [
        "# High-dimensional metrics comparison\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"HIGH-DIMENSIONAL ANALYSIS COMPARISON\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# K-Means metrics\n",
        "kmeans_sil_high = silhouette_score(X_high_scaled, kmeans_labels_high)\n",
        "kmeans_db_high = davies_bouldin_score(X_high_scaled, kmeans_labels_high)\n",
        "kmeans_ch_high = calinski_harabasz_score(X_high_scaled, kmeans_labels_high)\n",
        "\n",
        "# DBSCAN metrics (excluding noise)\n",
        "mask_high = dbscan_labels_high != -1\n",
        "if np.sum(mask_high) > 1 and n_clusters_high >= 2:\n",
        "    dbscan_sil_high = silhouette_score(X_high_scaled[mask_high], dbscan_labels_high[mask_high])\n",
        "    dbscan_db_high = davies_bouldin_score(X_high_scaled[mask_high], dbscan_labels_high[mask_high])\n",
        "    dbscan_ch_high = calinski_harabasz_score(X_high_scaled[mask_high], dbscan_labels_high[mask_high])\n",
        "else:\n",
        "    dbscan_sil_high = dbscan_db_high = dbscan_ch_high = 'N/A'\n",
        "\n",
        "comparison_high = pd.DataFrame({\n",
        "    'Metric': ['Silhouette Score', 'Davies-Bouldin', 'Calinski-Harabasz',\n",
        "               'Number of Clusters', 'Noise Points (%)'],\n",
        "    'K-Means': [f\"{kmeans_sil_high:.4f}\", f\"{kmeans_db_high:.4f}\", f\"{kmeans_ch_high:.2f}\",\n",
        "                OPTIMAL_K_HIGH, '0%'],\n",
        "    'DBSCAN': [f\"{dbscan_sil_high:.4f}\" if isinstance(dbscan_sil_high, float) else dbscan_sil_high,\n",
        "               f\"{dbscan_db_high:.4f}\" if isinstance(dbscan_db_high, float) else dbscan_db_high,\n",
        "               f\"{dbscan_ch_high:.2f}\" if isinstance(dbscan_ch_high, float) else dbscan_ch_high,\n",
        "               n_clusters_high, f\"{100*n_noise_high/len(dbscan_labels_high):.1f}%\"]\n",
        "})\n",
        "\n",
        "print(comparison_high.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"CURSE OF DIMENSIONALITY OBSERVATIONS:\")\n",
        "print(\"-\" * 70)\n",
        "print(\"1. DBSCAN struggles more in high dimensions (distance concentration)\")\n",
        "print(\"2. K-Means maintains stable performance with proper scaling\")\n",
        "print(f\"3. DBSCAN required larger epsilon ({best_params_high['eps'] if best_params_high else 'N/A'}) in high-D\")\n",
        "print(\"4. Noise percentage increased significantly in high-D DBSCAN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bFihUC_ihOT"
      },
      "source": [
        "---\n",
        "## 7. Quantum-Inspired Clustering\n",
        "\n",
        "### 7.1 Quantum Annealing-Inspired K-Means (QA-K-Means)\n",
        "\n",
        "We implement a quantum-inspired algorithm based on simulated quantum annealing principles for centroid optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:52.585384Z",
          "iopub.status.busy": "2026-01-26T17:40:52.585071Z",
          "iopub.status.idle": "2026-01-26T17:40:52.597464Z",
          "shell.execute_reply": "2026-01-26T17:40:52.595979Z"
        },
        "id": "PwoHi-8lihOT"
      },
      "outputs": [],
      "source": [
        "class QuantumInspiredKMeans:\n",
        "    \"\"\"\n",
        "    Quantum-Inspired K-Means using Simulated Quantum Annealing\n",
        "\n",
        "    This algorithm incorporates quantum-inspired mechanisms:\n",
        "    1. Quantum tunneling: Allows escape from local minima via probability-based jumps\n",
        "    2. Superposition-like initialization: Multiple centroid candidates\n",
        "    3. Interference-inspired update: Weighted combination of solutions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_clusters, max_iter=100, n_quantum_states=5,\n",
        "                 initial_temp=1.0, final_temp=0.01, random_state=42):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iter = max_iter\n",
        "        self.n_quantum_states = n_quantum_states  # Superposition cardinality\n",
        "        self.initial_temp = initial_temp\n",
        "        self.final_temp = final_temp\n",
        "        self.random_state = random_state\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    def _initialize_superposition(self, X):\n",
        "        \"\"\"Initialize multiple centroid candidates (superposition states)\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        centroids_list = []\n",
        "\n",
        "        for _ in range(self.n_quantum_states):\n",
        "            # K-means++ style initialization\n",
        "            idx = np.random.randint(n_samples)\n",
        "            centroids = [X[idx]]\n",
        "\n",
        "            for _ in range(1, self.n_clusters):\n",
        "                distances = np.min(cdist(X, np.array(centroids)), axis=1)\n",
        "                probs = distances ** 2\n",
        "                probs /= probs.sum()\n",
        "                idx = np.random.choice(n_samples, p=probs)\n",
        "                centroids.append(X[idx])\n",
        "\n",
        "            centroids_list.append(np.array(centroids))\n",
        "\n",
        "        return centroids_list\n",
        "\n",
        "    def _compute_energy(self, X, centroids):\n",
        "        \"\"\"Compute clustering energy (WCSS)\"\"\"\n",
        "        distances = cdist(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "        energy = np.sum(np.min(distances, axis=1) ** 2)\n",
        "        return energy, labels\n",
        "\n",
        "    def _quantum_tunneling(self, centroids, X, temperature):\n",
        "        \"\"\"Quantum tunneling: probabilistic jump to new state\"\"\"\n",
        "        new_centroids = centroids.copy()\n",
        "\n",
        "        # Tunneling probability based on temperature\n",
        "        tunnel_prob = np.exp(-1.0 / (temperature + 1e-10))\n",
        "\n",
        "        if np.random.random() < tunnel_prob:\n",
        "            # Perform quantum jump\n",
        "            k = np.random.randint(self.n_clusters)\n",
        "            # Jump to a random data point\n",
        "            idx = np.random.randint(len(X))\n",
        "            new_centroids[k] = X[idx] + np.random.normal(0, temperature, X.shape[1])\n",
        "\n",
        "        return new_centroids\n",
        "\n",
        "    def _interference_update(self, centroids_list, energies):\n",
        "        \"\"\"Interference-inspired weighted combination of solutions\"\"\"\n",
        "        # Compute amplitudes (inversely proportional to energy)\n",
        "        amplitudes = 1.0 / (np.array(energies) + 1e-10)\n",
        "        amplitudes /= amplitudes.sum()\n",
        "\n",
        "        # Weighted combination (constructive interference)\n",
        "        new_centroids = np.zeros_like(centroids_list[0])\n",
        "        for centroids, amp in zip(centroids_list, amplitudes):\n",
        "            new_centroids += amp * centroids\n",
        "\n",
        "        return new_centroids\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"Fit the quantum-inspired K-means algorithm\"\"\"\n",
        "        # Initialize superposition of states\n",
        "        centroids_list = self._initialize_superposition(X)\n",
        "\n",
        "        # Annealing schedule\n",
        "        temperatures = np.logspace(np.log10(self.initial_temp),\n",
        "                                   np.log10(self.final_temp),\n",
        "                                   self.max_iter)\n",
        "\n",
        "        best_centroids = None\n",
        "        best_energy = np.inf\n",
        "\n",
        "        self.energy_history = []\n",
        "\n",
        "        for iteration, temp in enumerate(temperatures):\n",
        "            energies = []\n",
        "\n",
        "            # Evolve each quantum state\n",
        "            for i, centroids in enumerate(centroids_list):\n",
        "                # Standard K-means update\n",
        "                energy, labels = self._compute_energy(X, centroids)\n",
        "\n",
        "                # Update centroids\n",
        "                new_centroids = np.array([X[labels == k].mean(axis=0)\n",
        "                                          if np.sum(labels == k) > 0 else centroids[k]\n",
        "                                          for k in range(self.n_clusters)])\n",
        "\n",
        "                # Apply quantum tunneling\n",
        "                new_centroids = self._quantum_tunneling(new_centroids, X, temp)\n",
        "\n",
        "                new_energy, _ = self._compute_energy(X, new_centroids)\n",
        "\n",
        "                # Metropolis criterion with quantum enhancement\n",
        "                if new_energy < energy or np.random.random() < np.exp(-(new_energy - energy) / temp):\n",
        "                    centroids_list[i] = new_centroids\n",
        "                    energy = new_energy\n",
        "\n",
        "                energies.append(energy)\n",
        "\n",
        "                # Track best solution\n",
        "                if energy < best_energy:\n",
        "                    best_energy = energy\n",
        "                    best_centroids = centroids_list[i].copy()\n",
        "\n",
        "            self.energy_history.append(best_energy)\n",
        "\n",
        "            # Interference: combine solutions periodically\n",
        "            if iteration % 10 == 0 and iteration > 0:\n",
        "                combined = self._interference_update(centroids_list, energies)\n",
        "                # Replace worst state with combined state\n",
        "                worst_idx = np.argmax(energies)\n",
        "                centroids_list[worst_idx] = combined\n",
        "\n",
        "        self.cluster_centers_ = best_centroids\n",
        "        _, self.labels_ = self._compute_energy(X, best_centroids)\n",
        "        self.inertia_ = best_energy\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict cluster labels\"\"\"\n",
        "        distances = cdist(X, self.cluster_centers_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "print(\"Quantum-Inspired K-Means class defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:52.600631Z",
          "iopub.status.busy": "2026-01-26T17:40:52.600382Z",
          "iopub.status.idle": "2026-01-26T17:40:53.492056Z",
          "shell.execute_reply": "2026-01-26T17:40:53.490563Z"
        },
        "id": "LIIjY4SgihOT"
      },
      "outputs": [],
      "source": [
        "# Apply Quantum-Inspired K-Means to univariate data\n",
        "print(\"Running Quantum-Inspired K-Means on Univariate Data...\")\n",
        "qi_kmeans = QuantumInspiredKMeans(n_clusters=OPTIMAL_K, max_iter=100,\n",
        "                                   n_quantum_states=5, random_state=42)\n",
        "qi_kmeans.fit(X_scaled)\n",
        "\n",
        "qi_labels = qi_kmeans.labels_\n",
        "qi_centroids = scaler.inverse_transform(qi_kmeans.cluster_centers_)\n",
        "\n",
        "print(f\"\\nQuantum-Inspired K-Means Results:\")\n",
        "print(f\"Final Inertia: {qi_kmeans.inertia_:.4f}\")\n",
        "print(f\"Silhouette Score: {silhouette_score(X_scaled, qi_labels):.4f}\")\n",
        "print(f\"\\nCluster Centroids:\")\n",
        "for i, c in enumerate(qi_centroids):\n",
        "    print(f\"  Cluster {i}: {c[0]:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:53.496130Z",
          "iopub.status.busy": "2026-01-26T17:40:53.495821Z",
          "iopub.status.idle": "2026-01-26T17:40:56.353807Z",
          "shell.execute_reply": "2026-01-26T17:40:56.351965Z"
        },
        "id": "Eadhw7cGihOT"
      },
      "outputs": [],
      "source": [
        "# Comparison visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Energy convergence\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(qi_kmeans.energy_history, 'b-', linewidth=2, label='QI-KMeans')\n",
        "ax1.axhline(y=qi_kmeans.inertia_, color='red', linestyle='--', label='Final Energy')\n",
        "ax1.set_xlabel('Iteration', fontsize=12)\n",
        "ax1.set_ylabel('Energy (WCSS)', fontsize=12)\n",
        "ax1.set_title('Quantum-Inspired K-Means Convergence', fontsize=14, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Cluster comparison - Standard K-Means\n",
        "ax2 = axes[0, 1]\n",
        "y_jitter = np.random.normal(0, 0.1, len(univariate_data))\n",
        "ax2.scatter(univariate_data, y_jitter, c=kmeans_labels, cmap='viridis', alpha=0.6, s=20)\n",
        "ax2.scatter(centroids_original, [0]*OPTIMAL_K, c='red', s=200, marker='X',\n",
        "            edgecolors='black', linewidths=2)\n",
        "ax2.set_title('Standard K-Means', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Alcohol Content (%)')\n",
        "\n",
        "# 3. Cluster comparison - QI K-Means\n",
        "ax3 = axes[1, 0]\n",
        "ax3.scatter(univariate_data, y_jitter, c=qi_labels, cmap='viridis', alpha=0.6, s=20)\n",
        "ax3.scatter(qi_centroids, [0]*OPTIMAL_K, c='red', s=200, marker='X',\n",
        "            edgecolors='black', linewidths=2)\n",
        "ax3.set_title('Quantum-Inspired K-Means', fontsize=14, fontweight='bold')\n",
        "ax3.set_xlabel('Alcohol Content (%)')\n",
        "\n",
        "# 4. Metric comparison\n",
        "ax4 = axes[1, 1]\n",
        "metrics = ['Silhouette', 'Davies-Bouldin\\n(inverted)']\n",
        "standard_vals = [silhouette_score(X_scaled, kmeans_labels),\n",
        "                  1/davies_bouldin_score(X_scaled, kmeans_labels)]\n",
        "qi_vals = [silhouette_score(X_scaled, qi_labels),\n",
        "           1/davies_bouldin_score(X_scaled, qi_labels)]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "ax4.bar(x - width/2, standard_vals, width, label='Standard K-Means', color='steelblue')\n",
        "ax4.bar(x + width/2, qi_vals, width, label='QI-KMeans', color='coral')\n",
        "ax4.set_ylabel('Score (Higher is Better)')\n",
        "ax4.set_title('Performance Comparison', fontsize=14, fontweight='bold')\n",
        "ax4.set_xticks(x)\n",
        "ax4.set_xticklabels(metrics)\n",
        "ax4.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('quantum_inspired_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:56.359451Z",
          "iopub.status.busy": "2026-01-26T17:40:56.359148Z",
          "iopub.status.idle": "2026-01-26T17:40:57.443511Z",
          "shell.execute_reply": "2026-01-26T17:40:57.442040Z"
        },
        "id": "NtEv-talihOT"
      },
      "outputs": [],
      "source": [
        "# Apply QI-KMeans to high-dimensional data\n",
        "print(\"\\nRunning Quantum-Inspired K-Means on High-Dimensional Data...\")\n",
        "qi_kmeans_high = QuantumInspiredKMeans(n_clusters=OPTIMAL_K_HIGH, max_iter=100,\n",
        "                                        n_quantum_states=5, random_state=42)\n",
        "qi_kmeans_high.fit(X_high_scaled)\n",
        "\n",
        "qi_labels_high = qi_kmeans_high.labels_\n",
        "\n",
        "print(f\"\\nQuantum-Inspired K-Means Results (High-D):\")\n",
        "print(f\"Final Inertia: {qi_kmeans_high.inertia_:.4f}\")\n",
        "print(f\"Silhouette Score: {silhouette_score(X_high_scaled, qi_labels_high):.4f}\")\n",
        "print(f\"Davies-Bouldin: {davies_bouldin_score(X_high_scaled, qi_labels_high):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxsrJ5qcihOU"
      },
      "source": [
        "### 7.2 Quantum-Inspired DBSCAN (QI-DBSCAN)\n",
        "\n",
        "We implement a quantum-inspired density-based clustering using probability amplitude-weighted density estimation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:57.446316Z",
          "iopub.status.busy": "2026-01-26T17:40:57.446054Z",
          "iopub.status.idle": "2026-01-26T17:40:57.455397Z",
          "shell.execute_reply": "2026-01-26T17:40:57.453917Z"
        },
        "id": "UiOkzKvoihOU"
      },
      "outputs": [],
      "source": [
        "class QuantumInspiredDBSCAN:\n",
        "    \"\"\"\n",
        "    Quantum-Inspired DBSCAN with probabilistic density estimation\n",
        "\n",
        "    Quantum-inspired features:\n",
        "    1. Wave function spread: Points have probabilistic neighborhoods\n",
        "    2. Entanglement: Connected points influence each other's density\n",
        "    3. Measurement: Final clustering collapses probabilities\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, eps, min_samples, quantum_spread=0.5, random_state=42):\n",
        "        self.eps = eps\n",
        "        self.min_samples = min_samples\n",
        "        self.quantum_spread = quantum_spread  # Wave function width\n",
        "        self.random_state = random_state\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    def _compute_quantum_density(self, X, point_idx):\n",
        "        \"\"\"Compute quantum-weighted density around a point\"\"\"\n",
        "        point = X[point_idx]\n",
        "        distances = np.linalg.norm(X - point, axis=1)\n",
        "\n",
        "        # Quantum amplitude: Gaussian wave function\n",
        "        amplitudes = np.exp(-distances**2 / (2 * self.quantum_spread**2))\n",
        "\n",
        "        # Density is sum of squared amplitudes (probability)\n",
        "        density = np.sum(amplitudes**2 * (distances <= self.eps))\n",
        "\n",
        "        return density, amplitudes\n",
        "\n",
        "    def _get_quantum_neighbors(self, X, point_idx, amplitudes):\n",
        "        \"\"\"Get neighbors with quantum probability weighting\"\"\"\n",
        "        distances = np.linalg.norm(X - X[point_idx], axis=1)\n",
        "\n",
        "        # Probabilistic neighborhood: sample based on amplitudes\n",
        "        probs = amplitudes**2\n",
        "        probs[point_idx] = 0  # Exclude self\n",
        "        probs = probs / probs.sum() if probs.sum() > 0 else probs\n",
        "\n",
        "        # Deterministic neighbors within eps\n",
        "        deterministic = np.where((distances <= self.eps) & (np.arange(len(X)) != point_idx))[0]\n",
        "\n",
        "        return deterministic\n",
        "\n",
        "    def fit_predict(self, X):\n",
        "        \"\"\"Fit and predict cluster labels\"\"\"\n",
        "        n_samples = len(X)\n",
        "        labels = np.full(n_samples, -1)  # -1 = unassigned\n",
        "\n",
        "        # Compute quantum densities\n",
        "        densities = np.zeros(n_samples)\n",
        "        amplitude_matrix = np.zeros((n_samples, n_samples))\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            densities[i], amplitude_matrix[i] = self._compute_quantum_density(X, i)\n",
        "\n",
        "        # Identify core points (quantum-enhanced density threshold)\n",
        "        quantum_threshold = self.min_samples * np.mean(densities) / np.max(densities)\n",
        "        core_mask = densities >= quantum_threshold\n",
        "        core_points = np.where(core_mask)[0]\n",
        "\n",
        "        cluster_id = 0\n",
        "\n",
        "        for point in core_points:\n",
        "            if labels[point] != -1:\n",
        "                continue\n",
        "\n",
        "            # Expand cluster\n",
        "            cluster = [point]\n",
        "            labels[point] = cluster_id\n",
        "\n",
        "            i = 0\n",
        "            while i < len(cluster):\n",
        "                current = cluster[i]\n",
        "                neighbors = self._get_quantum_neighbors(X, current, amplitude_matrix[current])\n",
        "\n",
        "                for neighbor in neighbors:\n",
        "                    if labels[neighbor] == -1:\n",
        "                        labels[neighbor] = cluster_id\n",
        "                        if core_mask[neighbor]:\n",
        "                            cluster.append(neighbor)\n",
        "\n",
        "                i += 1\n",
        "\n",
        "            cluster_id += 1\n",
        "\n",
        "        self.labels_ = labels\n",
        "        self.n_clusters_ = cluster_id\n",
        "        self.core_sample_indices_ = core_points\n",
        "\n",
        "        return labels\n",
        "\n",
        "print(\"Quantum-Inspired DBSCAN class defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:57.458365Z",
          "iopub.status.busy": "2026-01-26T17:40:57.457986Z",
          "iopub.status.idle": "2026-01-26T17:40:58.589238Z",
          "shell.execute_reply": "2026-01-26T17:40:58.587644Z"
        },
        "id": "exUYawWqihOU"
      },
      "outputs": [],
      "source": [
        "# Apply Quantum-Inspired DBSCAN\n",
        "print(\"Running Quantum-Inspired DBSCAN...\")\n",
        "qi_dbscan = QuantumInspiredDBSCAN(eps=EPS_OPTIMAL, min_samples=MIN_SAMPLES_OPTIMAL,\n",
        "                                   quantum_spread=0.3)\n",
        "qi_dbscan_labels = qi_dbscan.fit_predict(X_scaled)\n",
        "\n",
        "n_clusters_qi = len(set(qi_dbscan_labels)) - (1 if -1 in qi_dbscan_labels else 0)\n",
        "n_noise_qi = np.sum(qi_dbscan_labels == -1)\n",
        "\n",
        "print(f\"\\nQuantum-Inspired DBSCAN Results:\")\n",
        "print(f\"Number of clusters: {n_clusters_qi}\")\n",
        "print(f\"Noise points: {n_noise_qi} ({100*n_noise_qi/len(qi_dbscan_labels):.1f}%)\")\n",
        "\n",
        "if n_clusters_qi >= 2:\n",
        "    mask_qi = qi_dbscan_labels != -1\n",
        "    if np.sum(mask_qi) > 1:\n",
        "        print(f\"Silhouette Score (excl. noise): {silhouette_score(X_scaled[mask_qi], qi_dbscan_labels[mask_qi]):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:58.592662Z",
          "iopub.status.busy": "2026-01-26T17:40:58.592362Z",
          "iopub.status.idle": "2026-01-26T17:40:59.333071Z",
          "shell.execute_reply": "2026-01-26T17:40:59.331265Z"
        },
        "id": "__NMzNtbihOU"
      },
      "outputs": [],
      "source": [
        "# Final comprehensive comparison\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPREHENSIVE ALGORITHM COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "final_comparison = pd.DataFrame({\n",
        "    'Algorithm': ['K-Means', 'DBSCAN', 'QI-KMeans', 'QI-DBSCAN'],\n",
        "    'Clusters': [OPTIMAL_K, n_clusters_dbscan, OPTIMAL_K, n_clusters_qi],\n",
        "    'Noise (%)': ['0%', f'{100*n_noise/len(dbscan_labels):.1f}%', '0%', f'{100*n_noise_qi/len(qi_dbscan_labels):.1f}%'],\n",
        "    'Silhouette': [\n",
        "        f\"{silhouette_score(X_scaled, kmeans_labels):.4f}\",\n",
        "        f\"{silhouette_score(X_scaled[mask], dbscan_labels[mask]):.4f}\" if n_clusters_dbscan >= 2 else 'N/A',\n",
        "        f\"{silhouette_score(X_scaled, qi_labels):.4f}\",\n",
        "        f\"{silhouette_score(X_scaled[mask_qi], qi_dbscan_labels[mask_qi]):.4f}\" if n_clusters_qi >= 2 and np.sum(mask_qi) > 1 else 'N/A'\n",
        "    ],\n",
        "    'Type': ['Centroid-based', 'Density-based', 'Quantum-Centroid', 'Quantum-Density']\n",
        "})\n",
        "\n",
        "print(final_comparison.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrU3TJGKihOU"
      },
      "source": [
        "---\n",
        "## 8. Proposed Algorithm Extension\n",
        "\n",
        "### Adaptive Density K-Means (ADK-Means)\n",
        "\n",
        "A hybrid approach combining K-Means' efficiency with DBSCAN's density awareness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:40:59.336416Z",
          "iopub.status.busy": "2026-01-26T17:40:59.336077Z",
          "iopub.status.idle": "2026-01-26T17:40:59.997435Z",
          "shell.execute_reply": "2026-01-26T17:40:59.995916Z"
        },
        "id": "uduQA42IihOU"
      },
      "outputs": [],
      "source": [
        "class AdaptiveDensityKMeans:\n",
        "    \"\"\"\n",
        "    Adaptive Density K-Means (ADK-Means)\n",
        "\n",
        "    Combines K-Means with local density estimation to:\n",
        "    1. Identify and exclude outliers during clustering\n",
        "    2. Adapt cluster shapes based on local density\n",
        "    3. Provide more robust centroid estimates\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_clusters, outlier_percentile=5, max_iter=100, random_state=42):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.outlier_percentile = outlier_percentile\n",
        "        self.max_iter = max_iter\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def _estimate_local_density(self, X, k=10):\n",
        "        \"\"\"Estimate local density using k-NN distances\"\"\"\n",
        "        neighbors = NearestNeighbors(n_neighbors=k)\n",
        "        neighbors.fit(X)\n",
        "        distances, _ = neighbors.kneighbors(X)\n",
        "        # Density is inverse of average k-NN distance\n",
        "        avg_distances = distances.mean(axis=1)\n",
        "        densities = 1 / (avg_distances + 1e-10)\n",
        "        return densities\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"Fit ADK-Means\"\"\"\n",
        "        np.random.seed(self.random_state)\n",
        "        n_samples = len(X)\n",
        "\n",
        "        # Step 1: Estimate local densities\n",
        "        densities = self._estimate_local_density(X)\n",
        "\n",
        "        # Step 2: Identify outliers (low density points)\n",
        "        density_threshold = np.percentile(densities, self.outlier_percentile)\n",
        "        inlier_mask = densities >= density_threshold\n",
        "        X_inliers = X[inlier_mask]\n",
        "\n",
        "        # Step 3: Run K-Means on inliers with density-weighted centroids\n",
        "        density_weights = densities[inlier_mask]\n",
        "        density_weights = density_weights / density_weights.sum()\n",
        "\n",
        "        # K-means++ initialization\n",
        "        centroids = self._kmeans_plus_plus_init(X_inliers, density_weights)\n",
        "\n",
        "        # Iterate\n",
        "        for _ in range(self.max_iter):\n",
        "            # Assign labels (weighted by density)\n",
        "            distances = cdist(X_inliers, centroids)\n",
        "            labels_inliers = np.argmin(distances, axis=1)\n",
        "\n",
        "            # Update centroids with density weighting\n",
        "            new_centroids = np.zeros_like(centroids)\n",
        "            for k in range(self.n_clusters):\n",
        "                cluster_mask = labels_inliers == k\n",
        "                if np.sum(cluster_mask) > 0:\n",
        "                    weights = density_weights[cluster_mask]\n",
        "                    weights = weights / weights.sum()\n",
        "                    new_centroids[k] = np.average(X_inliers[cluster_mask],\n",
        "                                                   weights=weights, axis=0)\n",
        "                else:\n",
        "                    new_centroids[k] = centroids[k]\n",
        "\n",
        "            # Check convergence\n",
        "            if np.allclose(centroids, new_centroids):\n",
        "                break\n",
        "            centroids = new_centroids\n",
        "\n",
        "        self.cluster_centers_ = centroids\n",
        "\n",
        "        # Step 4: Assign all points (including outliers)\n",
        "        all_distances = cdist(X, centroids)\n",
        "        self.labels_ = np.argmin(all_distances, axis=1)\n",
        "\n",
        "        # Mark outliers\n",
        "        self.outlier_mask_ = ~inlier_mask\n",
        "        self.labels_with_outliers_ = self.labels_.copy()\n",
        "        self.labels_with_outliers_[self.outlier_mask_] = -1\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _kmeans_plus_plus_init(self, X, weights):\n",
        "        \"\"\"Density-weighted K-means++ initialization\"\"\"\n",
        "        n_samples = len(X)\n",
        "\n",
        "        # First centroid: highest density point\n",
        "        first_idx = np.argmax(weights)\n",
        "        centroids = [X[first_idx]]\n",
        "\n",
        "        for _ in range(1, self.n_clusters):\n",
        "            distances = np.min(cdist(X, np.array(centroids)), axis=1)\n",
        "            probs = (distances ** 2) * weights\n",
        "            probs = probs / probs.sum()\n",
        "            next_idx = np.random.choice(n_samples, p=probs)\n",
        "            centroids.append(X[next_idx])\n",
        "\n",
        "        return np.array(centroids)\n",
        "\n",
        "# Apply ADK-Means\n",
        "print(\"Running Adaptive Density K-Means...\")\n",
        "adk = AdaptiveDensityKMeans(n_clusters=OPTIMAL_K, outlier_percentile=5)\n",
        "adk.fit(X_scaled)\n",
        "\n",
        "n_outliers_adk = np.sum(adk.outlier_mask_)\n",
        "print(f\"\\nADK-Means Results:\")\n",
        "print(f\"Clusters: {OPTIMAL_K}\")\n",
        "print(f\"Outliers detected: {n_outliers_adk} ({100*n_outliers_adk/len(X_scaled):.1f}%)\")\n",
        "print(f\"Silhouette (all points): {silhouette_score(X_scaled, adk.labels_):.4f}\")\n",
        "\n",
        "non_outlier_mask = adk.labels_with_outliers_ != -1\n",
        "if np.sum(non_outlier_mask) > 1:\n",
        "    print(f\"Silhouette (excl. outliers): {silhouette_score(X_scaled[non_outlier_mask], adk.labels_with_outliers_[non_outlier_mask]):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-26T17:41:00.000125Z",
          "iopub.status.busy": "2026-01-26T17:40:59.999840Z",
          "iopub.status.idle": "2026-01-26T17:41:00.005144Z",
          "shell.execute_reply": "2026-01-26T17:41:00.003945Z"
        },
        "id": "cKPIyrOjihOV"
      },
      "outputs": [],
      "source": [
        "# Save all figures and results\n",
        "print(\"\\nAnalysis Complete!\")\n",
        "print(\"\\nGenerated Figures:\")\n",
        "print(\"  - eda_univariate.png\")\n",
        "print(\"  - kmeans_k_selection.png\")\n",
        "print(\"  - kmeans_results_univariate.png\")\n",
        "print(\"  - dbscan_epsilon_selection.png\")\n",
        "print(\"  - dbscan_results_univariate.png\")\n",
        "print(\"  - kmeans_high_dimensional.png\")\n",
        "print(\"  - dbscan_high_dimensional.png\")\n",
        "print(\"  - quantum_inspired_comparison.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LxZeFUmihOf"
      },
      "source": [
        "---\n",
        "## Summary and Conclusions\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **Univariate Analysis**: The alcohol content in white wines shows multimodal distribution suitable for clustering. K-Means with K=3 provided clear separation, while DBSCAN identified outliers in extreme alcohol ranges.\n",
        "\n",
        "2. **High-Dimensional Analysis**: Both algorithms face challenges in 11-dimensional space. K-Means maintained stability, while DBSCAN required careful parameter tuning due to the curse of dimensionality.\n",
        "\n",
        "3. **Quantum-Inspired Approaches**: QI-KMeans showed improved escape from local minima through quantum tunneling mechanisms. QI-DBSCAN provided softer density boundaries.\n",
        "\n",
        "4. **Proposed Extension**: The Adaptive Density K-Means (ADK-Means) successfully combines the efficiency of K-Means with density-based outlier detection.\n",
        "\n",
        "### Recommendations\n",
        "\n",
        "- Use **K-Means** when cluster shapes are approximately spherical and the number of clusters is known\n",
        "- Use **DBSCAN** when clusters have arbitrary shapes and outlier detection is important\n",
        "- Consider **quantum-inspired** methods for complex optimization landscapes\n",
        "- The **ADK-Means** hybrid offers a practical balance for real-world applications"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}